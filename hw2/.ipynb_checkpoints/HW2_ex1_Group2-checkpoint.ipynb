{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import zlib\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "version = \"a\"\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "column_indices = [2, 5]\n",
    "columns = df.columns[column_indices]\n",
    "data = df[columns].values.astype(np.float32)\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[0:int(n*0.7)]\n",
    "val_data = data[int(n*0.7):int(n*0.9)]\n",
    "test_data = data[int(n*0.9):]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "input_width = 6\n",
    "if version == \"a\":\n",
    "    output_steps = 3\n",
    "if version == \"b\":\n",
    "    output_steps = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to deal with windows in the tempeorature and humidity forecasting\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, output_steps, mean, std):\n",
    "        self.input_width = input_width\n",
    "        self.output_steps = output_steps\n",
    "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])\n",
    "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, :self.input_width, :]\n",
    "        labels = features[:, -self.output_steps:, :]\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, 2])\n",
    "        labels.set_shape([None, self.output_steps, 2])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def normalize(self, features):\n",
    "        features = (features - self.mean) / (self.std + 1.e-6)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def preprocess(self, features):\n",
    "        inputs, labels = self.split_window(features)\n",
    "        inputs = self.normalize(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, train):\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length=input_width+self.output_steps,\n",
    "                sequence_stride=1,\n",
    "                batch_size=32)\n",
    "        ds = ds.map(self.preprocess)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to deal with two values of MAE (temperature and humidity)\n",
    "class MultiOutputMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_absolute_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight('total', initializer='zeros', shape=(2,))\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_pred - y_true)\n",
    "        error = tf.reduce_mean(error, axis=[0,1])\n",
    "        self.total.assign_add(error)\n",
    "        self.count.assign_add(1.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.count.assign(tf.zeros_like(self.count))\n",
    "        self.total.assign(tf.zeros_like(self.total))\n",
    "\n",
    "    def result(self):\n",
    "        result = tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate quantized models\n",
    "def load_and_evaluation(path, dataset):\n",
    "    interpreter = tf.lite.Interpreter(model_path = path) \n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    dataset = dataset.unbatch().batch(1)\n",
    "    \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        my_input = np.array(data[0], dtype = np.float32)\n",
    "        label = np.array(data[1], dtype = np.float32)\n",
    "        labels.append(label)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], my_input)\n",
    "        interpreter.invoke()\n",
    "        my_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        outputs.append(my_output[0])\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    labels = np.squeeze(np.array(labels))\n",
    "    \n",
    "    mae = np.sum(np.sum(np.absolute(outputs - labels), axis = 0), axis = 0)/(labels.shape[0]*3)\n",
    "    return mae\n",
    "\n",
    "# Function for weight and activations quantization \n",
    "def representative_dataset_generator():\n",
    "    for x, _ in train_ds.take(1000):\n",
    "        yield [x]\n",
    "\n",
    "generator = WindowGenerator(input_width, output_steps, mean, std)\n",
    "train_ds = generator.make_dataset(train_data, True)\n",
    "val_ds = generator.make_dataset(val_data, False)\n",
    "test_ds = generator.make_dataset(test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models (version a and b)\n",
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(input_width, 2), name='flatten'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='dense2'),\n",
    "    tf.keras.layers.Dense(units = 2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(input_shape=(input_width, 2), filters=64, kernel_size=3, activation='relu', name='convolution'),\n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(units=2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(input_shape=(input_width, 2), units=64, name='lstm'),\n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    tf.keras.layers.Dense(units=2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "# Select model to train (from input)\n",
    "MODELS = {'c': mlp, 'b': cnn, 'a': lstm}\n",
    "model = MODELS[version]\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = [MultiOutputMAE()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training without optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9200/9200 [==============================] - 46s 5ms/step - loss: 180.9820 - mean_absolute_error: 6.2657 - val_loss: 4.4057 - val_mean_absolute_error: 1.3782\n",
      "Epoch 2/5\n",
      "9200/9200 [==============================] - 43s 5ms/step - loss: 2.8472 - mean_absolute_error: 1.0288 - val_loss: 2.1665 - val_mean_absolute_error: 0.8439\n",
      "Epoch 3/5\n",
      "9200/9200 [==============================] - 42s 5ms/step - loss: 2.1931 - mean_absolute_error: 0.8654 - val_loss: 2.0501 - val_mean_absolute_error: 0.7791\n",
      "Epoch 4/5\n",
      "9200/9200 [==============================] - 40s 4ms/step - loss: 2.0358 - mean_absolute_error: 0.8103 - val_loss: 2.0031 - val_mean_absolute_error: 0.7785\n",
      "Epoch 5/5\n",
      "9200/9200 [==============================] - 42s 5ms/step - loss: 1.9774 - mean_absolute_error: 0.7859 - val_loss: 2.3625 - val_mean_absolute_error: 0.8792\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                17152     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 6)                 390       \n",
      "                                                                 \n",
      " reshape_28 (Reshape)        (None, 3, 2)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,542\n",
      "Trainable params: 17,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "1314/1314 [==============================] - 3s 2ms/step - loss: 2.4963 - mean_absolute_error: 0.9299\n",
      "Test error:  [0.40383202 1.4559175 ]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
    "print(model.summary())\n",
    "\n",
    "# Test model\n",
    "test_loss, test_error = model.evaluate(test_ds)\n",
    "print('Test error: ', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/no_optimization/Group2_th_a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/no_optimization/Group2_th_a/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f046b5f3b90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "# Save the model on disk\n",
    "if not os.path.exists('./models/no_optimization/'):\n",
    "    os.makedirs('./models/no_optimization/')\n",
    "\n",
    "run_model = tf.function(lambda x: model(x))\n",
    "concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
    "saving_path = os.path.join('.','models', 'no_optimization','Group2_th_{}'.format(version))\n",
    "model.save(saving_path, signatures=concrete_func)\n",
    "\n",
    "# Conert model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saving_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_model_dir = os.path.join('.','models', 'no_optimization', 'Group2_th_{}.tflite'.format(version))\n",
    "\n",
    "with open(tflite_model_dir, 'wb') as fp:\n",
    "    fp.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training with pruning**\n",
    "\n",
    "**1) Structured Pruning via Width Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5 #[0,1]\n",
    "# Models (version a and b)\n",
    "pruned_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(input_width, 2), name='flatten'),\n",
    "    tf.keras.layers.Dense(int(128*alpha), activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(int(128*alpha), activation='relu', name='dense2'),\n",
    "    tf.keras.layers.Dense(units = int(2*output_steps), name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "pruned_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(input_shape = (input_width, 2), filters=int(64*alpha), kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=int(64*alpha), activation='relu'),\n",
    "    tf.keras.layers.Dense(units=int(2*output_steps)),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "pruned_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(input_shape=(input_width, 2), units=int(64*alpha), name='lstm'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=int(2*output_steps), name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "# Select model to train (from input)\n",
    "PRUNED_MODELS = {'b': pruned_mlp, 'c': pruned_cnn, 'a': pruned_lstm}\n",
    "model = PRUNED_MODELS[version]\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = [MultiOutputMAE()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Magnitude-based Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  aggregation=tf.VariableAggregation.MEAN)\n",
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:207: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  aggregation=tf.VariableAggregation.MEAN)\n",
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:220: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  trainable=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9200/9200 [==============================] - 34s 4ms/step - loss: 807.5862 - mean_absolute_error: 19.7157 - val_loss: 90.1414 - val_mean_absolute_error: 7.5404\n",
      "Epoch 2/5\n",
      "9200/9200 [==============================] - 39s 4ms/step - loss: 50.5676 - mean_absolute_error: 5.2634 - val_loss: 33.2496 - val_mean_absolute_error: 4.0370\n",
      "Epoch 3/5\n",
      "9200/9200 [==============================] - 39s 4ms/step - loss: 39.8646 - mean_absolute_error: 4.3032 - val_loss: 29.7499 - val_mean_absolute_error: 3.8801\n",
      "Epoch 4/5\n",
      "9200/9200 [==============================] - 39s 4ms/step - loss: 27.9401 - mean_absolute_error: 3.4807 - val_loss: 16.7892 - val_mean_absolute_error: 2.6911\n",
      "Epoch 5/5\n",
      "9200/9200 [==============================] - 38s 4ms/step - loss: 17.9863 - mean_absolute_error: 2.4783 - val_loss: 13.6324 - val_mean_absolute_error: 2.2314\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_lstm (P  (None, 32)               8835      \n",
      " runeLowMagnitude)                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 32)               1         \n",
      " _7 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_output_  (None, 6)                392       \n",
      " layer (PruneLowMagnitude)                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_reshape  (None, 3, 2)             1         \n",
      " _31 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,229\n",
      "Trainable params: 4,678\n",
      "Non-trainable params: 4,551\n",
      "_________________________________________________________________\n",
      "None\n",
      "1314/1314 [==============================] - 2s 1ms/step - loss: 15.4070 - mean_absolute_error: 2.3605\n",
      "Test error:  [3.2498312 1.4710885]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "end_step = np.ceil(len(train_ds) / 32).astype(np.int32) * epochs\n",
    "\n",
    "pruning_params = {'pruning_schedule':\n",
    "    tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.3,\n",
    "        final_sparsity=0.8,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "        )\n",
    "    }\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Define the pruning callback\n",
    "callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "\n",
    "# Train the model\n",
    "input_shape = [32, 6, 2]\n",
    "model.build(input_shape)\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[callbacks])\n",
    "print(model.summary())\n",
    "\n",
    "test_loss, test_error = model.evaluate(test_ds)\n",
    "print('Test error: ', test_error)\n",
    "\n",
    "# Strip the model\n",
    "model = tfmot.sparsity.keras.strip_pruning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp88_dr1or/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp88_dr1or/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model size: 9.88kB\n"
     ]
    }
   ],
   "source": [
    "# Save pruned model \n",
    "if not os.path.exists('./models/pruned/'):\n",
    "    os.makedirs('./models/pruned/')\n",
    "    \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model_dir = os.path.join('.','models', 'pruned', 'Group2_th_{}_pruned.tflite'.format(version))\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_model_dir, 'wb') as fp:\n",
    "    tflite_compressed = zlib.compress(tflite_model)\n",
    "    fp.write(tflite_compressed)\n",
    "\n",
    "# Size of the quantized model (weights only)\n",
    "print('Pruned model size: {:.2f}kB'.format(os.path.getsize(tflite_model_dir)/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization on trained models\n",
    "\n",
    "**1) Weights-only PTQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw5dq_eze/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw5dq_eze/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size (weights only): 9.88kB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model provided has model identifier 'y\\MY', should be 'TFL3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-12dbc18fdfdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Evaluation of the PTQ model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MAE quantized model (weight and activations)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-9e82c218b3af>\u001b[0m in \u001b[0;36mload_and_evaluation\u001b[0;34m(path, dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and evaluate quantized models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml4iot-env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    463\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[1;32m    464\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[1;32m    466\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to open {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Model provided has model identifier 'y\\MY', should be 'TFL3'\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "if not os.path.exists('./models/quantized/'):\n",
    "    os.makedirs('./models/quantized/')\n",
    "\n",
    "quantized_model_dir = os.path.join('.', 'models', 'quantized', 'Group2_th_{}_quantized_weights_only.tflite'.format(version))\n",
    "with open(quantized_model_dir, 'wb') as fp:\n",
    "    tflite_compressed = zlib.compress(tflite_model)\n",
    "    fp.write(tflite_compressed)\n",
    "\n",
    "# Size of the quantized model (weights only)\n",
    "print('Quantized model size (weights only): {:.2f}kB'.format(os.path.getsize(quantized_model_dir)/1000))\n",
    "\n",
    "# Evaluation of the PTQ model\n",
    "mae = load_and_evaluation(quantized_model_dir, test_ds)\n",
    "print('MAE quantized model (weight and activations)', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Weights+Activations PTQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3298s6n0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3298s6n0/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size (weights and activations): 9.80kB\n",
      "MAE quantized model (weight and activations) [0.37418936 1.46184594]\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_generator\n",
    "#converter.target_spec.supported_ops = [\n",
    "#    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n",
    "#]\n",
    "tflite_quantized_model2 = converter.convert()\n",
    "\n",
    "quantized_model_dir2 = os.path.join('.', 'models', 'quantized', 'Group2_th_{}_quantized_wa'.format(version))\n",
    "\n",
    "with open(quantized_model_dir2, 'wb') as fp:\n",
    "        fp.write(tflite_quantized_model2)\n",
    "\n",
    "# Size of the quantized model (weights and activation)\n",
    "print('Quantized model size (weights and activations): {:.2f}kB'.format(os.path.getsize(quantized_model_dir2)/1000))\n",
    "\n",
    "# Evaluation of the PTQ model\n",
    "mae = load_and_evaluation(quantized_model_dir2, test_ds)\n",
    "print('MAE quantized model (weight and activations)', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
