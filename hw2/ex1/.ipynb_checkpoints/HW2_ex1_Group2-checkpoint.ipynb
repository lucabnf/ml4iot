{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import zlib\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "version = \"a\"\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "column_indices = [2, 5]\n",
    "columns = df.columns[column_indices]\n",
    "data = df[columns].values.astype(np.float32)\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[0:int(n*0.7)]\n",
    "val_data = data[int(n*0.7):int(n*0.9)]\n",
    "test_data = data[int(n*0.9):]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "input_width = 6\n",
    "if version == \"a\":\n",
    "    output_steps = 3\n",
    "if version == \"b\":\n",
    "    output_steps = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to deal with windows in the tempeorature and humidity forecasting\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, output_steps, mean, std):\n",
    "        self.input_width = input_width\n",
    "        self.output_steps = output_steps\n",
    "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])\n",
    "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, :self.input_width, :]\n",
    "        labels = features[:, -self.output_steps:, :]\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, 2])\n",
    "        labels.set_shape([None, self.output_steps, 2])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def normalize(self, features):\n",
    "        features = (features - self.mean) / (self.std + 1.e-6)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def preprocess(self, features):\n",
    "        inputs, labels = self.split_window(features)\n",
    "        inputs = self.normalize(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, train):\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length=input_width+self.output_steps,\n",
    "                sequence_stride=1,\n",
    "                batch_size=32)\n",
    "        ds = ds.map(self.preprocess)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to deal with two values of MAE (temperature and humidity)\n",
    "class MultiOutputMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_absolute_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight('total', initializer='zeros', shape=(2,))\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_pred - y_true)\n",
    "        error = tf.reduce_mean(error, axis=[0,1])\n",
    "        self.total.assign_add(error)\n",
    "        self.count.assign_add(1.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.count.assign(tf.zeros_like(self.count))\n",
    "        self.total.assign(tf.zeros_like(self.total))\n",
    "\n",
    "    def result(self):\n",
    "        result = tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate quantized models\n",
    "def load_and_evaluation(path, dataset):\n",
    "    f = open(path, 'rb')\n",
    "    decompressed_model = zlib.decompress(f.read())\n",
    "    interpreter = tf.lite.Interpreter(model_content=decompressed_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # set batch size to 1 when running inference with TFLite models\n",
    "    dataset = dataset.unbatch().batch(1)\n",
    "    \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        my_input = np.array(data[0], dtype = np.float32)\n",
    "        label = np.array(data[1], dtype = np.float32)\n",
    "        labels.append(label)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], my_input)\n",
    "        interpreter.invoke()\n",
    "        my_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        outputs.append(my_output[0])\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    labels = np.squeeze(np.array(labels))\n",
    "    \n",
    "    mae = np.sum(np.sum(np.absolute(outputs - labels), axis = 0), axis = 0)/(labels.shape[0]*3)\n",
    "    return mae\n",
    "\n",
    "# Function for weight and activations quantization \n",
    "def representative_dataset_generator():\n",
    "    for x, _ in train_ds.take(1000):\n",
    "        yield [x]\n",
    "\n",
    "generator = WindowGenerator(input_width, output_steps, mean, std)\n",
    "train_ds = generator.make_dataset(train_data, True)\n",
    "val_ds = generator.make_dataset(val_data, False)\n",
    "test_ds = generator.make_dataset(test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Models (version a and b)\n",
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(input_width, 2), name='flatten'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='dense2'),\n",
    "    tf.keras.layers.Dense(units = 2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(input_shape=(input_width, 2), filters=64, kernel_size=3, activation='relu', name='convolution'),\n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(units=2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(input_shape=(input_width, 2), units=64, name='lstm'),\n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    tf.keras.layers.Dense(units=2*output_steps, name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "# Select model to train (from input)\n",
    "MODELS = {'c': mlp, 'b': cnn, 'a': lstm}\n",
    "model = MODELS[version]\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = [MultiOutputMAE()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training without optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
    "print(model.summary())\n",
    "\n",
    "# Test model\n",
    "test_loss, test_error = model.evaluate(test_ds)\n",
    "print('Test error: ', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save the model on disk\n",
    "if not os.path.exists('./models/no_optimization/'):\n",
    "    os.makedirs('./models/no_optimization/')\n",
    "\n",
    "run_model = tf.function(lambda x: model(x))\n",
    "concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
    "saving_path = os.path.join('.','models', 'no_optimization','Group2_th_{}'.format(version))\n",
    "model.save(saving_path, signatures=concrete_func)\n",
    "\n",
    "# Conert model to tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saving_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_model_dir = os.path.join('.','models', 'no_optimization', 'Group2_th_{}.tflite'.format(version))\n",
    "\n",
    "with open(tflite_model_dir, 'wb') as fp:\n",
    "    fp.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training with pruning**\n",
    "\n",
    "**1) Structured Pruning via Width Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.03 #[0,1]\n",
    "# Models (version a and b)\n",
    "pruned_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(input_width, 2), name='flatten'),\n",
    "    tf.keras.layers.Dense(int(128*alpha), activation='relu', name='dense1'),\n",
    "    tf.keras.layers.Dense(int(128*alpha), activation='relu', name='dense2'),\n",
    "    tf.keras.layers.Dense(units = int(2*output_steps), name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "pruned_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(input_shape = (input_width, 2), filters=int(64*alpha), kernel_size=3, activation='relu'),    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=int(2*output_steps)),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "pruned_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(input_shape=(input_width, 2), units=int(64*alpha), name='lstm'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=int(2*output_steps), name='output_layer'),\n",
    "    tf.keras.layers.Reshape([output_steps, 2])\n",
    "])\n",
    "\n",
    "# Select model to train (from input)\n",
    "PRUNED_MODELS = {'a': pruned_mlp, 'b': pruned_cnn, 'c': pruned_lstm}\n",
    "model = PRUNED_MODELS[version]\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = [MultiOutputMAE()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        t = logs[\"mean_absolute_error\"][0]\n",
    "        h = logs[\"mean_absolute_error\"][1]\n",
    "        print(\"\\t T mae={:.3f}, H mae={:.3f}\".format(t, h))\n",
    "\n",
    "mycallback = CustomCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Magnitude-based Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:220: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  trainable=False)\n",
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:200: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  aggregation=tf.VariableAggregation.MEAN)\n",
      "/home/luca/ml4iot-env/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:207: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  aggregation=tf.VariableAggregation.MEAN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9189/9200 [============================>.] - ETA: 0s - loss: 2.1213 - mean_absolute_error: 0.8090\t T mae=0.435, H mae=1.184\n",
      "9200/9200 [==============================] - 31s 3ms/step - loss: 2.1229 - mean_absolute_error: 0.8093 - val_loss: 1.9968 - val_mean_absolute_error: 0.7663\n",
      "Epoch 2/50\n",
      "9196/9200 [============================>.] - ETA: 0s - loss: 2.0874 - mean_absolute_error: 0.7942\t T mae=0.409, H mae=1.179\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 2.0880 - mean_absolute_error: 0.7943 - val_loss: 1.9600 - val_mean_absolute_error: 0.7428\n",
      "Epoch 3/50\n",
      "9188/9200 [============================>.] - ETA: 0s - loss: 2.0546 - mean_absolute_error: 0.7823\t T mae=0.389, H mae=1.176\n",
      "9200/9200 [==============================] - 25s 3ms/step - loss: 2.0570 - mean_absolute_error: 0.7827 - val_loss: 1.9656 - val_mean_absolute_error: 0.7621\n",
      "Epoch 4/50\n",
      "9198/9200 [============================>.] - ETA: 0s - loss: 2.0337 - mean_absolute_error: 0.7732\t T mae=0.371, H mae=1.175\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 2.0333 - mean_absolute_error: 0.7731 - val_loss: 1.9424 - val_mean_absolute_error: 0.7273\n",
      "Epoch 5/50\n",
      "9192/9200 [============================>.] - ETA: 0s - loss: 2.0155 - mean_absolute_error: 0.7674\t T mae=0.359, H mae=1.176\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 2.0163 - mean_absolute_error: 0.7676 - val_loss: 1.9231 - val_mean_absolute_error: 0.7242\n",
      "Epoch 6/50\n",
      "9199/9200 [============================>.] - ETA: 0s - loss: 2.0024 - mean_absolute_error: 0.7618\t T mae=0.348, H mae=1.175\n",
      "9200/9200 [==============================] - 29s 3ms/step - loss: 2.0023 - mean_absolute_error: 0.7618 - val_loss: 1.9129 - val_mean_absolute_error: 0.7140\n",
      "Epoch 7/50\n",
      "9192/9200 [============================>.] - ETA: 0s - loss: 1.9869 - mean_absolute_error: 0.7563\t T mae=0.338, H mae=1.175\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9883 - mean_absolute_error: 0.7565 - val_loss: 1.9159 - val_mean_absolute_error: 0.7176\n",
      "Epoch 8/50\n",
      "9181/9200 [============================>.] - ETA: 0s - loss: 1.9785 - mean_absolute_error: 0.7525\t T mae=0.331, H mae=1.174\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 1.9783 - mean_absolute_error: 0.7526 - val_loss: 1.9312 - val_mean_absolute_error: 0.7286\n",
      "Epoch 9/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.9658 - mean_absolute_error: 0.7477\t T mae=0.324, H mae=1.172\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 1.9679 - mean_absolute_error: 0.7480 - val_loss: 1.9159 - val_mean_absolute_error: 0.7228\n",
      "Epoch 10/50\n",
      "9189/9200 [============================>.] - ETA: 0s - loss: 1.9593 - mean_absolute_error: 0.7446\t T mae=0.319, H mae=1.171\n",
      "9200/9200 [==============================] - 28s 3ms/step - loss: 1.9589 - mean_absolute_error: 0.7446 - val_loss: 1.8958 - val_mean_absolute_error: 0.7056\n",
      "Epoch 11/50\n",
      "9186/9200 [============================>.] - ETA: 0s - loss: 1.9540 - mean_absolute_error: 0.7439\t T mae=0.315, H mae=1.173\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9549 - mean_absolute_error: 0.7440 - val_loss: 1.9249 - val_mean_absolute_error: 0.7244\n",
      "Epoch 12/50\n",
      "9195/9200 [============================>.] - ETA: 0s - loss: 1.9476 - mean_absolute_error: 0.7401\t T mae=0.310, H mae=1.170\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9474 - mean_absolute_error: 0.7401 - val_loss: 1.9082 - val_mean_absolute_error: 0.7063\n",
      "Epoch 13/50\n",
      "9184/9200 [============================>.] - ETA: 0s - loss: 1.9386 - mean_absolute_error: 0.7376\t T mae=0.307, H mae=1.169\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9397 - mean_absolute_error: 0.7379 - val_loss: 1.9058 - val_mean_absolute_error: 0.7133\n",
      "Epoch 14/50\n",
      "9189/9200 [============================>.] - ETA: 0s - loss: 1.9364 - mean_absolute_error: 0.7361\t T mae=0.304, H mae=1.168\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9365 - mean_absolute_error: 0.7362 - val_loss: 1.8919 - val_mean_absolute_error: 0.7158\n",
      "Epoch 15/50\n",
      "9195/9200 [============================>.] - ETA: 0s - loss: 1.9297 - mean_absolute_error: 0.7346\t T mae=0.303, H mae=1.167\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 1.9316 - mean_absolute_error: 0.7348 - val_loss: 1.8788 - val_mean_absolute_error: 0.6934\n",
      "Epoch 16/50\n",
      "9187/9200 [============================>.] - ETA: 0s - loss: 1.9293 - mean_absolute_error: 0.7348\t T mae=0.302, H mae=1.168\n",
      "9200/9200 [==============================] - 26s 3ms/step - loss: 1.9295 - mean_absolute_error: 0.7348 - val_loss: 1.9076 - val_mean_absolute_error: 0.7129\n",
      "Epoch 17/50\n",
      "9185/9200 [============================>.] - ETA: 0s - loss: 1.9219 - mean_absolute_error: 0.7312\t T mae=0.298, H mae=1.164\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9215 - mean_absolute_error: 0.7313 - val_loss: 1.8960 - val_mean_absolute_error: 0.7151\n",
      "Epoch 18/50\n",
      "9196/9200 [============================>.] - ETA: 0s - loss: 1.9189 - mean_absolute_error: 0.7308\t T mae=0.296, H mae=1.165\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9193 - mean_absolute_error: 0.7309 - val_loss: 1.9181 - val_mean_absolute_error: 0.7272\n",
      "Epoch 19/50\n",
      "9190/9200 [============================>.] - ETA: 0s - loss: 1.9196 - mean_absolute_error: 0.7307\t T mae=0.296, H mae=1.165\n",
      "9200/9200 [==============================] - 27s 3ms/step - loss: 1.9189 - mean_absolute_error: 0.7307 - val_loss: 1.8996 - val_mean_absolute_error: 0.7076\n",
      "Epoch 20/50\n",
      "9185/9200 [============================>.] - ETA: 0s - loss: 1.9133 - mean_absolute_error: 0.7292\t T mae=0.294, H mae=1.164\n",
      "9200/9200 [==============================] - 29s 3ms/step - loss: 1.9141 - mean_absolute_error: 0.7293 - val_loss: 1.8929 - val_mean_absolute_error: 0.7150\n",
      "1314/1314 [==============================] - 2s 1ms/step - loss: 2.0330 - mean_absolute_error: 0.7665\n",
      "Test error:  [0.3104353 1.222645 ]\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "#end_step = np.ceil(len(train_ds) / 32).astype(np.int32) * epochs\n",
    "\n",
    "pruning_params = {'pruning_schedule':\n",
    "    tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.25,\n",
    "        final_sparsity=0.5,\n",
    "        begin_step=2*len(train_ds),\n",
    "        end_step=20*len(train_ds)\n",
    "        )\n",
    "    }\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Early stopping callback\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "# Define the pruning callback\n",
    "callbacks = [tfmot.sparsity.keras.UpdatePruningStep(), mycallback, es_callback]\n",
    "\n",
    "# Train the model\n",
    "input_shape = [32, 6, 2]\n",
    "model.build(input_shape)\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n",
    "\n",
    "test_loss, test_error = model.evaluate(test_ds)\n",
    "print('Test error: ', test_error)\n",
    "\n",
    "# Strip the model\n",
    "model = tfmot.sparsity.keras.strip_pruning(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization on trained models\n",
    "\n",
    "**1) Weights-only PTQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Weights+Activations PTQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_generator\n",
    "#converter.target_spec.supported_ops = [\n",
    "#    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n",
    "#]\n",
    "tflite_quantized_model2 = converter.convert()\n",
    "\n",
    "quantized_model_dir2 = os.path.join('.', 'models', 'quantized', 'Group2_th_{}_quantized_wa'.format(version))\n",
    "\n",
    "with open(quantized_model_dir2, 'wb') as fp:\n",
    "        fp.write(tflite_quantized_model2)\n",
    "\n",
    "# Size of the quantized model (weights and activation)\n",
    "print('Quantized model size (weights and activations): {:.2f}kB'.format(os.path.getsize(quantized_model_dir2)/1000))\n",
    "\n",
    "# Evaluation of the PTQ model\n",
    "mae = load_and_evaluation(quantized_model_dir2, test_ds)\n",
    "print('MAE quantized model (weight and activations)', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Quantization-aware problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization-aware model\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "model = quantize_model(model)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "# es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=5)\n",
    "\n",
    "# Train the model\n",
    "input_shape = [32, 6, 2]\n",
    "model.build(input_shape)\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.fit(train_ds, epochs=1, validation_data=val_ds)\n",
    "\n",
    "test_loss, test_error = q_aware_model.evaluate(test_ds)\n",
    "print('Test error: ', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpai3aup8r/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpai3aup8r/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.45kB\n",
      "MAE quantized model: [t_mae=0.310, h_mae=1.223]\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "if not os.path.exists('./models/prova/'):\n",
    "    os.makedirs('./models/prova/')\n",
    "\n",
    "model_dir = os.path.join('models', 'prova', 'Group2_th_1.tflite')\n",
    "\n",
    "with open(model_dir, 'wb') as fp:\n",
    "    tflite_compressed = zlib.compress(tflite_model)\n",
    "    fp.write(tflite_compressed)\n",
    "    \n",
    "# Size of the quantized model (weights only)\n",
    "print('Model size: {:.2f}kB'.format(os.path.getsize(model_dir)/1000))\n",
    "\n",
    "# Evaluation of the PTQ model\n",
    "mae = load_and_evaluation(model_dir, test_ds)\n",
    "print('MAE quantized model: [t_mae={:.3f}, h_mae={:.3f}]'.format(mae[0], mae[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
